{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01c83b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7096fd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"./results/checkpoint-10950\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32ddcdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.983989953994751, 'start': 31, 'end': 36, 'answer': 'Paris'}\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "    \"context\": \"The Eiffel Tower is located in Paris and is one of the most visited landmarks in the world.\",\n",
    "    \"question\": \"Where is the Eiffel Tower located?\"\n",
    "}\n",
    "print(qa_pipeline(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a42e473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570\n"
     ]
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\")\n",
    "val_set = squad[\"validation\"]\n",
    "print(len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "33c3f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c6a4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 77.07663197729423, 'f1': 85.11510430622388}\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"squad\")\n",
    "\n",
    "preds, refs = [], []\n",
    "\n",
    "for i in range(len(val_set)):\n",
    "    ex = val_set[i]\n",
    "    pred = qa_pipeline({\"context\": ex[\"context\"], \"question\": ex[\"question\"]})\n",
    "    preds.append({\"id\": ex[\"id\"], \"prediction_text\": pred[\"answer\"]})\n",
    "    refs.append({\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]})\n",
    "\n",
    "result = metric.compute(predictions=preds, references=refs)\n",
    "print(result)\n",
    "\n",
    "# Add EM and F1 into your results dict\n",
    "results[\"Exact Match\"] = round(result[\"exact_match\"], 2)\n",
    "results[\"F1 Score\"] = round(result[\"f1\"], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed55c3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency: 26.60 ms\n",
      "Throughput: 37.59 questions/sec\n"
     ]
    }
   ],
   "source": [
    "test_ex = {\n",
    "    \"context\": val_set[0][\"context\"],\n",
    "    \"question\": val_set[0][\"question\"]\n",
    "}\n",
    "\n",
    "qa_pipeline(test_ex)\n",
    "\n",
    "runs = 20\n",
    "start = time.time()\n",
    "for _ in range(runs):\n",
    "    qa_pipeline(test_ex)\n",
    "end = time.time()\n",
    "\n",
    "avg_time = (end - start) / runs\n",
    "latency_ms = avg_time * 1000\n",
    "throughput = 1 / avg_time\n",
    "\n",
    "print(f\"Average latency: {latency_ms:.2f} ms\")\n",
    "print(f\"Throughput: {throughput:.2f} questions/sec\")\n",
    "\n",
    "# Store results for later presentation\n",
    "results[\"latency\"] = round(latency_ms, 2)\n",
    "results[\"throughput\"] = round(throughput, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e62d7517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 760.5 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_model_size(model_dir=checkpoint):\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(model_dir):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return round(total_size / (1024*1024), 2)\n",
    "\n",
    "model_size_mb = get_model_size(checkpoint)\n",
    "print(\"Model size:\", model_size_mb, 'MB')\n",
    "\n",
    "results['Model_size'] = model_size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2732de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics: {'Exact Match': 77.08, 'F1 Score': 85.12, 'latency': 26.6, 'throughput': 37.59, 'Model_size': 760.5}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"M1_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Saved metrics:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8af6fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_mem = round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2)\n",
    "\n",
    "results['GPU'] = f\"GPU: {gpu_name} ({gpu_mem} GB VRAM)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ace5f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned DistilBERT Evaluation:\n",
      "Model Size: 760.5 MB\n",
      "F1 Score: 85.12\n",
      "Exact Match: 77.08\n",
      "Latency: 26.6 ms\n",
      "Throughput: 37.59 Questions per Second\n",
      "GPU: GPU: NVIDIA GeForce GTX 1650 (4.0 GB VRAM)\n"
     ]
    }
   ],
   "source": [
    "print(f'Finetuned DistilBERT Evaluation:')\n",
    "print(f\"Model Size: {results['Model_size']} MB\")\n",
    "print(f\"F1 Score: {results['F1 Score']}\")\n",
    "print(f\"Exact Match: {results['Exact Match']}\")\n",
    "print(f\"Latency: {results['latency']} ms\")\n",
    "print(f\"Throughput: {results['throughput']} Questions per Second\")\n",
    "print(f\"GPU: {results['GPU']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
