{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9d053e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kuchh\\anaconda3\\envs\\mlp_bert\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05da43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ae3446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5705ea7975f01819005e775c',\n",
       " 'title': 'New_Delhi',\n",
       " 'context': \"The first major extension of New Delhi outside of Lutyens' Delhi came in the 1950s when the Central Public Works Department (CPWD) developed a large area of land southwest of Lutyens' Delhi to create the diplomatic enclave of Chanakyapuri, where land was allotted for embassies, chanceries, high commissions and residences of ambassadors, around wide central vista, Shanti Path.\",\n",
       " 'question': 'What was the name of the enclave created by the Central Public Works Department?',\n",
       " 'answers': {'text': ['Chanakyapuri'], 'answer_start': [226]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['train'][24566]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9819817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b8d1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69717d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff2527e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(example):\n",
    "#     return tokenizer(\n",
    "#         example[\"question\"],\n",
    "#         example[\"context\"],\n",
    "#         truncation=\"only_second\",\n",
    "#         max_length=384,\n",
    "#         stride=128,\n",
    "#         padding=\"max_length\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# tokenized_squad = squad.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04fb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    start_positions, end_positions = [], []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        answer = examples[\"answers\"][i]\n",
    "        \n",
    "        # Handle empty answers (SQuAD v2 or broken entries)\n",
    "        if len(answer[\"answer_start\"]) == 0 or len(answer[\"text\"]) == 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "\n",
    "        idx_start, idx_end = None, None\n",
    "        for idx, (s, e) in enumerate(offsets):\n",
    "            if s <= start_char < e:\n",
    "                idx_start = idx\n",
    "            if s < end_char <= e:\n",
    "                idx_end = idx\n",
    "\n",
    "        # If we can’t find start or end → mark as CLS token (0)\n",
    "        if idx_start is None or idx_end is None:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_positions.append(idx_start)\n",
    "            end_positions.append(idx_end)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",       \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,          \n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c21b01a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenization complete!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_squad = squad.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"✅ Tokenization complete!\")\n",
    "print(tokenized_squad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46942d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Torch version: 2.7.1+cu118\n",
      "Built with CUDA: 11.8\n",
      "GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Built with CUDA:\", torch.version.cuda)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28694de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA devices: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA devices:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ae94040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuchh\\AppData\\Local\\Temp\\ipykernel_18584\\751887242.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 3:13:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.452300</td>\n",
       "      <td>1.123431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.883500</td>\n",
       "      <td>1.096142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10950, training_loss=1.1679125196204339, metrics={'train_runtime': 11590.4708, 'train_samples_per_second': 15.116, 'train_steps_per_second': 0.945, 'total_flos': 1.7167621364554752e+16, 'train_loss': 1.1679125196204339, 'epoch': 2.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65bf1680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./final_model\\\\tokenizer_config.json',\n",
       " './final_model\\\\special_tokens_map.json',\n",
       " './final_model\\\\vocab.txt',\n",
       " './final_model\\\\added_tokens.json',\n",
       " './final_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./final_model\")  \n",
    "tokenizer.save_pretrained(\"./final_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
